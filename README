prll
version 0.4.9999
===========

prll (pronounced "parallel") is a utility for use with bash or zsh. It
provides a convenient interface for parallelizing the execution of a
single task over multiple data files, or actually any kind of data
that you can pass as a function argument. It is meant to make it
simple to fully utilize a multicore/multiprocessor machine.

Homepage: http://prll.sourceforge.net/


DESCRIPTION
===========

prll is designed to be used not just in shell scripts, but especially
in interactive shells. To make the latter convenient, it is
implemented as a shell function. Shells are not much good at automatic
job management; see [1] for further discussion. Therefore, prll uses
helper programs, written in C. To prevent race conditions, System V
Message Queues and Semaphores are used to signal job completion. It
also features full output buffering to prevent mangling of data
because of concurrent output.

[1] http://prll.sourceforge.net/shell_parallel.html


REQUIREMENTS
============

  - bash or zsh
  - C compiler, such as gcc
  - GNU make
  - OS support for System V Message Queues and Semaphores
  - device files /dev/urandom or /dev/random
  - the cat utility
  - optional tests require utilites tr, grep, sort, split, diff and uname

Systems tested: GNU/Linux, FreeBSD, OpenBSD, MacOS X

These requirements should be satisfied by your system by default,
excepting perhaps the compiler and its toolchain, which are not
installed by default on systems such as Ubuntu Linux. Refer to your
system's documentation on how to install missing programs.

prll also looks for the /proc/cpuinfo file. It uses it to
automatically determine the number of processors. Non-Linux systems
may lack this file or have a different syntax. Setting the
PRLL_NR_CPUS environment variable renders the cpuinfo file unnecessary
(see usage instructions below).

Linux systems that were tested had much larger Message Queue size than
non-Linux systems. Queue size dictates the maximum number of jobs that
can be run in parallel. A rule of thumb: a BSD system can run about 20
jobs, depending on the number of existing queues, while a Linux system
can run over 500 jobs; at that point, the shell's job table becomes
saturated.


INSTALLATION
============

Compile prll_qer.c. You can use the included Makefile. If you
have gcc, you can simply run

  make

On non-GNU systems, replace that with 'gmake'.
If you have gcc and want different compiler options, do

  CFLAGS=whatever make

If you have a different compiler, you may want to completely override
compiler options, like so

  make CFLAGS=whatever

You may wish to run 'make test' at this point. It will do some simple
tests to verify that prll produces correct results. The test suite is
not comprehensive, however, and is itself not well tested, so you
should try prll on real-world data before coming to any
conclusions. Failure is not necessarily meaningful, either. For
example, if there are already several message queues being used on
your system, you may be running out of IPC memory. That would cause
some tests to fail or hang (in fact, some are disabled on non-Linux
systems for this reason), while it wouldn't interfere with normal
operation as long as you don't run too many parallel jobs. Also, be
aware that full testing requires several hundred megabytes of disk
space.

When prll is built, copy the prll_qer and prll_bfr executables to a
directory you have in your PATH. For example, to do a system-wide
installation, run as root

  chown root:root prll_qer prll_bfr
  cp prll_qer prll_bfr /usr/local/bin/

File prll.sh contains the shell function. The shell that will use it
needs to source it. That means that:
  - If you wish to use prll in a shell script, simply copy it in there.
  - If you wish to use prll in an interactive shell, source it.
The latter means that you need to put the function somewhere where
your shell will find it. If you are installing it for yourself, put
it in your .bashrc or .zshrc. If you are installing it system-wide, put
it in /etc/profile. However, if your system has the /etc/profile.d
directory, use that. For example

  chown root:root prll.sh
  cp prll.sh /etc/profile.d/

The function should now be automatically sourced by login shells. To
have every shell source it instead of only login shells, insert

  source /etc/profile

into your .bashrc or .zshrc.


USAGE
=====

Synopsis: prll [-b] function_name arguments ...
	  prll [-b] -s 'code' arguments ...

Order of options is important.

Summary of options:
-------------------

  function_name:   Name of the shell function to be executed.
  arguments:	   List of arguments that function_name should be
  		   executed over.

  -b	Disable output buffering.
  -s	Take a string of shell code instead of a shell function name.
  -p	Read arguments as lines from standard input instead of command
  	line. This option must be used at the end of command line
  	(i.e. instead of the argument list).
  -0	Same as -p, but use the null character as delimiter instead of
  	newline.

Operation:
----------

To execute a task, create a shell function that does something to its
first argument. Pass that function to prll along with the arguments
you wish to execute it on.

As an alternative, you may pass the '-s' flag, followed by a
string. The string will be executed as if it were the body of a shell
function. Therefore, you may use $1 to reference its first argument.
Be sure to quote the string properly to prevent shell expansion.

Instead of arguments, you can use flags '-p' or '-0'. prll will then
take its arguments from stdin. The '-p' flag will make it read lines
and the '-0' flag will make it read null-delimited input. This mode
emulates the 'xargs' utility a bit, but is easier for interactive use
because xargs makes it hard to pass complex commands. Reading large
arguments (such as lines several kilobytes long) in this fashion is
slow, however. If your data comes in such large chunks, it is much
faster to split it into several files and pass a list of those to prll
instead.

The '-b' option disables output buffering. See below for explanation.
Alternatively, buffering may be disabled by setting the PRLL_BUFFER
environment variable to "no".

The number of tasks to be run in parallel is provided via the
PRLL_NR_CPUS environment variable. If it is not provided, prll will
look into the /proc/cpuinfo file and extract the number of CPUs in
your computer.

Aborting execution and cleanup:
-------------------------------

If you need to abort execution, you can do it with the usual Ctrl+C
key combination. prll will wait for remaining jobs to complete before
exiting. You can force termination by typing Ctrl+C once more. The
jobs' PIDs are printed during execution so you can easily track them
down and terminate them if necessary. prll will not do it
automatically because only you can decide whether they are still doing
useful work or not. Also, jobs print completion notifications on their
own so you will know when they are done even if prll itself is
terminated. Should that be the case, they will also print a harmless
'file not found' message because the message queue will be
removed. But be aware that if you force prll to terminate by using
Ctrl+C twice, the semaphore used to synchronise buffers will be
removed along with the message queue and your jobs will be unable to
print anything. This is not a problem when jobs don't write to
standard output or when buffering is disabled.

The command 'prll_interrupt' is available from within your
functions. It causes prll to abort execution in the same way as
described above.

prll cleans after itself, but if you are worried about stale message
queues or semaphores, you can list them with the 'ipcs' command and
remove them with the 'ipcrm' command. Refer to your system's
documentation for details.

Buffering:
----------

Transport of data between programs is normally buffered by the
operating system. These buffers are small (e.g. 4kB on Linux), but are
enough to enhance performance. Multiple programs writing to the same
destination, as is the case with prll, is then arranged like this:

  +-----+    +-----------+
  | job |--->| OS buffer |\
  +-----+    +-----------+ \
			    \
  +-----+    +-----------+   \+-------------+
  | job |--->| OS buffer |--->| Output/File |
  +-----+    +-----------+   /+-------------+
			    /
  +-----+    +-----------+ /
  | job |--->| OS buffer |/
  +-----+    +-----------+

The output can be passed to another program, over a network or into a
file. But the jobs run in parallel, so the question is: what will the
data they produce look like at the destination when they write it at
the same time?

If a job writes less data than the size of the OS buffer, then
everything is fine: the buffer is never filled and the OS flushes it
when the job exits. All output from that job is in one piece because
the OS will flush only one buffer at a time.

If, however, a job writes more data than that, then the OS flushes the
buffer each time it is filled. Because several jobs run in parallel,
their outputs become interleaved at the destination, which is not
good.

prll does additional job output buffering by default. The actual
arrangement when running prll looks like this:

  +-----+    +-----------+    +-------------+
  | job |--->| OS buffer |--->| prll buffer |\
  +-----+    +-----------+    +-------------+ \
			       	     | 	       \
  +-----+    +-----------+    +-------------+   \+-------------+
  | job |--->| OS buffer |--->| prll buffer |--->| Output/File |
  +-----+    +-----------+    +-------------+   /+-------------+
			       	     | 	       /
  +-----+    +-----------+    +-------------+ /
  | job |--->| OS buffer |--->| prll buffer |/
  +-----+    +-----------+    +-------------+

Note the vertical connections between prll buffers: they synchronise
so that they only write data to the destination one at a time. They
make sure that all of the output of a single job is in one piece. To
keep performance high, the jobs must keep running, therefore each
buffer must be able to keep taking in data, even if it cannot
immediately write it. To make this possible, prll buffers aren't
limited in size: they grow to accomodate all data a job produces.

This raises another concern: you need to have enough memory to contain
the data until it can be written. If your jobs produce more data than
you have memory, you need to redirect it to files. Have each job
create a file and redirect all its output to that file. You can do
that however you want, but there should be a helpful utility available
on your sysyem: 'mktemp'. It is dedicated to creating files with
unique names.

As noted at the beginning of this section, prll's additional buffering
can be disabled. It is not necessary to do this when each job writes
to its own file. It is meant to be used as a safety measure. prll was
written with interactive use in mind, and when writing functions on
the fly, it can easily happen that an error creeps in. If an error
causes spurious output (e.g. if the function gets stuck in an infinite
loop) it can easily waste a lot of memory. The option to disable
buffering is meant to be used when you believe that your jobs should
only produce a small amount of data, but aren't sure that they
actually will.

It should be noted that buffering only applies to standard
output. OS buffers standard error differently (i.e. by lines) and prll
does nothing to change that.


Examples:
---------

Suppose you have a set of photos that you wish to process using the
'mogrify' utility. Simply do

  function myfn() { mogrify -flip $1 ; }
  prll myfn *.jpg

This will run mogrify on each jpg file in the current directory. If
your computer has 4 processors, but you wish to run only 3 tasks at
once, you should use

  PRLL_NR_CPUS=3 prll myfn *.jpg

Or, to make it permanent in the current shell, do

  PRLL_NR_CPUS=3

on a line of its own. You don't need to export the variable because
prll automatically has access to everything your shell can see.


To make things shorter, you can leave out the 'function' keyword when
defining a function. To redefine the same example:

  myfn() { mogrify -flip $1 ; }

However, you now need to be careful not to forget the parentheses. When
using the 'function' keyword, they are optional. If you leave it out,
they are not.

All examples here are very short. In practice, it is quicker to pass
such a short function on the command line directly:

  prll -s 'mogrify -flip $1' *.jpg

prll now automatically wraps the code in an internal function so you
don't have to. Don't forget about the single quotes, or the shell will
expand $1 before prll is run.


If you have a more complicated function that has to take more than one
argument, you should use a trick: combine multiple arguments into one
when passing them to prll, then split them again inside your
function. Here is a stub for a function that takes three arguments:

  # for zsh only
  function myfn() { echo $1 | read a b c; process $a; compute $b; kill $c; }
  # for both bash and zsh
  function myfn() { read a b c <<<$1 ; process $a; compute $b; kill $c; }
  prll myfn 'a1 b1 c1' 'a2 b2 c3' 'a3 b3 c3' ...

If you have even more complex requirements, you can use the '-0'
option and pipe null-delimited data into prll.


You may wish to abort execution if one of the results is wrong. In
that case, use something like this:

     function myfn() { compute $1; [[ $result == "wrong" ]] && prll_interrupt; }


If you have many arguments to process, it might be easier to pipe them
to standard input. Suppose each line of a file is an argument of its
own. Simply pipe the file into prll:

  function myfn() { some; processing | goes && on; here; }
  cat file_with_arguments | prll myfn -p > results


KNOWN ISSUES
------------

This section describes issues and bugs that were known at the time of
release. For more current information, check the newer version of this
file. You can find it in the project's git repository, also accessible
directly using a web browser:

  http://prll.git.sourceforge.net/git/gitweb.cgi?p=prll/prll;a=tree

There, you will find out which new issues were found and which of the
old issues were fixed. If you have an issue that is not described,
check the git log if it was ever found and fixed. If not, please
contact the author. Suggestions are welcome, patches even more so.


Known issues:

- Reading standard input is slow.
  This is mostly fine because it only gets annoying when reading
  rather large chunks, and in such a case, it is reasonable to split
  them into separate files. It is only really inconvenient when you
  have a huge file which you cannot afford to split. Implementing a
  faster internal splitting on stdin is quite possible, but difficult
  to do elegantly. It will only be considered if there is a genuine
  need for it.

- Expand the test suite.
  Specifically, termination behaviour on external interrupt signal
  currently has to be checked manually. Also, checking of stderr
  output is not done.

- Document cross-compilation and make it easier to cross-compile.

- Shell's job table becomes saturated with a large number of jobs.
  This is not really an issue, since it happens when the number of
  jobs is above 500 or so. Nevertheless, it might be possible to
  disown jobs if such a large number of them should be required.


LICENSING INFORMATION
---------------------

The prll package is provided under the GNU General Public
License, version 3 or later. See COPYING for more information.

Copyright 2009-2010 Jure Varlec
